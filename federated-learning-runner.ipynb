{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\n\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn import utils\nfrom sklearn.metrics import accuracy_score\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\nfrom time import time\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2021-07-15T09:46:58.992680Z","iopub.execute_input":"2021-07-15T09:46:58.993015Z","iopub.status.idle":"2021-07-15T09:47:03.810152Z","shell.execute_reply.started":"2021-07-15T09:46:58.992988Z","shell.execute_reply":"2021-07-15T09:47:03.809348Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Model: CNN\nclass CNN:\n    def initiate(self, n_classes):\n        model = Sequential([\n              ## Convolutional Layers\n              Conv2D(filters=32, kernel_size=(5,5), padding='same', input_shape=(28,28,1)),\n              MaxPooling2D(pool_size=(2,2)),\n              Conv2D(filters=64, kernel_size=(5,5), padding='same'), \n              MaxPooling2D(pool_size=(2,2)),\n\n              ## Fully Connected Layer\n              Flatten(),\n              Dense(512, activation='relu'), \n              Dense(n_classes, activation='softmax')\n            ])\n\n        return model\n    \n# Model: 2NN\nclass NN2Layers:\n    def initiate(self, n_classes):\n        model = Sequential([\n                            Flatten(input_shape=(28,28)), \n                            Dense(200, activation='relu'), \n                            Dense(200, activation='relu'), \n                            Dense(n_classes, activation='softmax')\n        ])\n\n        return model","metadata":{"execution":{"iopub.status.busy":"2021-07-15T09:47:03.811538Z","iopub.execute_input":"2021-07-15T09:47:03.811849Z","iopub.status.idle":"2021-07-15T09:47:03.821020Z","shell.execute_reply.started":"2021-07-15T09:47:03.811814Z","shell.execute_reply":"2021-07-15T09:47:03.820129Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Generate data based on chosen setting\ndef generate_data(setting, n_clients):\n     \n    # Scale image to [0,1]\n    def scale_image(input_array):\n        input_array = input_array / 255.0\n        \n        return input_array\n    \n    # Load data from Tensorflow\n    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n    \n    # Scale data\n    x_train, x_test = scale_image(x_train), scale_image(x_test)\n\n    # Expand dims to add batch axis\n    x_train = np.expand_dims(x_train, axis=-1)\n    x_test = np.expand_dims(x_test, axis=-1)\n    \n    # Distribute data based on setting\n    if setting == \"IID\":\n        # Shuffle train data\n        x_train_shuffled, y_train_shuffled = utils.shuffle(x_train, y_train, random_state=21)\n        \n        # Partitioned proxy data to clients, each receiving 600 examples\n        clients_data, clients_label = {}, {}\n        list_data = np.array_split(x_train_shuffled, n_clients)\n        list_label = np.array_split(y_train_shuffled, n_clients)\n\n        # Distribute the data to all clients\n        for i in range(1, len(list_data)+1):\n            clients_data[\"client_%s\" % i] = list_data[i-1]\n            clients_label[\"client_%s\" % i] = list_label[i-1]\n            \n    return clients_data, clients_label, x_train, y_train, x_test, y_test\n\n# Initiate global model\ndef initiate_model(model_fam, num_class):\n    if model_fam == \"2NN\":\n        nn = NN2Layers()\n        active_nn = nn.initiate(num_class)\n    elif model_fam == \"CNN\":\n        nn = CNN()\n        active_nn = nn.initiate(num_class)\n        \n    return active_nn\n\n# Scaling weights to the dataset proportion\ndef scale_weights(num_local_samples, weights, chosen_clients):\n    num_total_samples = len(chosen_clients) * num_local_samples\n    scaling_factor = num_local_samples / num_total_samples\n\n    # Loop through each layer weight & biases\n    scaled_weights = []\n    for component in weights:\n        scaled_weights.append(scaling_factor * component)\n\n    return scaled_weights\n\n# Sum all the scaled weights from all clients\ndef sum_scaled_weights(scaled_weights):\n    final_weights = []\n    for component in zip(*scaled_weights):\n        final_weights.append(tf.math.reduce_sum(component, axis=0))\n  \n    return final_weights\n\n# Custom global model evaluation\ndef evaluate_model(model, test_data, test_label):\n    y_pred = model.predict(test_data)\n    y_true = test_label\n\n    # Calculate loss with SCCE\n    scce = tf.keras.losses.SparseCategoricalCrossentropy()\n    loss = scce(y_true, y_pred).numpy()\n\n    # Calculate accuracy\n    accuracy = accuracy_score(y_true, np.argmax(y_pred, axis=1))\n\n    return round(loss, 4), round(accuracy, 4)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T09:47:03.823049Z","iopub.execute_input":"2021-07-15T09:47:03.823829Z","iopub.status.idle":"2021-07-15T09:47:03.837602Z","shell.execute_reply.started":"2021-07-15T09:47:03.823764Z","shell.execute_reply":"2021-07-15T09:47:03.836822Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def run_experiment(model_family, setting, b_size, epoch=1,\\\n                   est_comm_round=1000, c_fraction=0.1, desired_acc=False, run_until=False,\\\n                   model_checkpoint=False, n_clients=100, n_classes=10,\\\n                   learning_rate=0.01, output_path='./logs/'):\n    \n    # Training setting\n    EXP_NAME = \"%s-B%s-C%s-MNIST-%s\" % (setting, b_size, c_fraction, model_family)\n    start_time = time()\n    history = []\n    comm_round = 1\n    \n    # Generate data\n    clients_data, clients_label, x_train, y_train, x_test, y_test = generate_data(setting,\\\n                                                                                  n_clients)\n    \n    # Initiate new global model\n    if model_checkpoint == False: \n        global_nn = initiate_model(model_family, n_classes)\n        \n    # Continue training from existing checkpoint\n    else:\n        global_nn = tf.keras.models.load_model(model_checkpoint)\n        \n    # Training setting\n    comm_round = 1\n    start_time = time()\n    \n    # Loop until estimated communication round reached\n    for i in tqdm(range(est_comm_round)):\n\n        # Save the global weight\n        global_weights = global_nn.get_weights()\n\n        # Take c random subset client\n        if c_fraction == 0.0:\n            chosen_clients = np.random.choice(list(clients_data.keys()), 1)\n        else:\n            chosen_clients = np.random.choice(list(clients_data.keys()), \\\n                                          int(c_fraction*len(clients_data)))\n\n        clients_weight = [] \n        for client in chosen_clients:\n            # Iniatiate local model\n            local_nn = initiate_model(model_family, n_classes)\n            optimizer = tf.keras.optimizers.SGD(\n                learning_rate=learning_rate)\n            local_nn.compile(\n                optimizer=optimizer,\n                loss='sparse_categorical_crossentropy',\n                metrics=['accuracy']\n                )\n\n            # Set global weight to the local model\n            local_nn.set_weights(global_weights)\n\n            # Do training on local\n            local_nn.fit(clients_data[client], clients_label[client], batch_size=b_size,\\\n                         epochs=epoch, verbose=0)\n\n            # Save weight\n            scaled_weights = scale_weights(len(clients_data[client]), local_nn.get_weights(),\\\n                                           chosen_clients)\n            clients_weight.append(scaled_weights)\n\n        # Sum all scaled weights & update the global model\n        global_nn.set_weights(sum_scaled_weights(clients_weight))\n\n        # Evaluate the loss & accuracy\n        train_loss, train_accuracy = evaluate_model(global_nn, x_train, y_train)\n        test_loss, test_accuracy = evaluate_model(global_nn, x_test, y_test)\n\n        # Save metrics for current round\n        data = {\"C\": c_fraction, \"B\": b_size, \"comm_round\": comm_round, \"train_acc\": train_accuracy, \n              \"test_acc\": test_accuracy, \"train_loss\": train_loss, \"test_loss\": test_loss}\n        \n        # Save model state and history\n        history.append(data)\n        global_nn.save(output_path + EXP_NAME)\n        file = open(output_path + EXP_NAME + \".txt\", 'a')\n        file.write(json.dumps(str(data)) + \"\\n\")\n        file.close()\n        print(data)\n\n        # Stop when the desired test-accuracy reached, comm_round ignored\n        if desired_acc != False and test_accuracy >= desired_acc:\n            break\n\n        # Update variables\n        comm_round += 1\n    end_time = time()\n    print(\"Took %.2f seconds.\" % (end_time - start_time))\n    return history","metadata":{"execution":{"iopub.status.busy":"2021-07-15T09:47:03.840349Z","iopub.execute_input":"2021-07-15T09:47:03.840704Z","iopub.status.idle":"2021-07-15T09:47:03.857237Z","shell.execute_reply.started":"2021-07-15T09:47:03.840676Z","shell.execute_reply":"2021-07-15T09:47:03.856458Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Run experiment with your own setting\nhistory = run_experiment(model_family=\"CNN\", setting=\"IID\", b_size=50, epoch=20,\\\n                         est_comm_round=1000, c_fraction=0.1, desired_acc=False,\\\n                         model_checkpoint=False, n_clients=100, n_classes=10,\\\n                         learning_rate=0.01, output_path='./logs/')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}