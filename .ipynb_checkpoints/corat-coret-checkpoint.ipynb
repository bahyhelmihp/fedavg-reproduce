{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "443151a2-3dcd-46d6-bf57-3f1c86ad5d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import utils\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from time import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9362577b-a4d9-4d74-9bf7-b5f2043b3df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data based on chosen setting\n",
    "def generate_data(data, use_val, setting, n_clients):\n",
    "     \n",
    "    # Scale image to [0,1]\n",
    "    def scale_image(input_array):\n",
    "        input_array = input_array / 255.0\n",
    "        \n",
    "        return input_array\n",
    "    \n",
    "    # Load data from Tensorflow Library\n",
    "    if data == \"MNIST\":\n",
    "        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "    \n",
    "    # Scale data\n",
    "    x_train, x_test = scale_image(x_train), scale_image(x_test)\n",
    "\n",
    "    # Expand dims to add batch axis\n",
    "    x_train = np.expand_dims(x_train, axis=-1)\n",
    "    x_test = np.expand_dims(x_test, axis=-1)\n",
    "    \n",
    "    # Divide train into val\n",
    "    if use_val == True:\n",
    "        x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=len(x_test), random_state=42)\n",
    "    else:\n",
    "        x_val, y_val = None, None\n",
    "    \n",
    "    # Distribute data based on setting\n",
    "    if setting == \"IID\":\n",
    "        # Shuffle train data\n",
    "        x_train_shuffled, y_train_shuffled = utils.shuffle(x_train, y_train, random_state=21)\n",
    "        \n",
    "        # Partitioned proxy data to clients, each receiving 600 examples\n",
    "        clients_data, clients_label = {}, {}\n",
    "        list_data = np.array_split(x_train_shuffled, n_clients)\n",
    "        list_label = np.array_split(y_train_shuffled, n_clients)\n",
    "\n",
    "        # Distribute the data to all clients\n",
    "        for i in range(1, len(list_data)+1):\n",
    "            clients_data[\"client_%s\" % i] = list_data[i-1]\n",
    "            clients_label[\"client_%s\" % i] = list_label[i-1]\n",
    "            \n",
    "    elif setting == \"NONIID\":\n",
    "        # Sort the train data based on digits label\n",
    "        sorted_index = np.argsort(y_train)\n",
    "        y_train = y_train[sorted_index]\n",
    "        x_train = x_train[sorted_index]\n",
    "        \n",
    "        # Split training data into to distribute to 100 clients\n",
    "        data_shards = np.array_split(x_train, 100)\n",
    "        label_shards = np.array_split(y_train, 100)\n",
    "        \n",
    "        # Distribute shards to all clients\n",
    "        N_CLIENTS = 100\n",
    "        clients_data = {}\n",
    "        clients_label = {}\n",
    "        for i in range(1, N_CLIENTS+1):\n",
    "            clients_data[\"client_%s\" % i] = data_shards[i-1]\n",
    "            clients_label[\"client_%s\" % i] = label_shards[i-1]\n",
    "            \n",
    "    return clients_data, clients_label, x_train, y_train, x_val, y_val, x_test, y_test\n",
    "\n",
    "# Initiate global model\n",
    "def initiate_model(model_fam, num_class):\n",
    "    if model_fam == \"2NN\":\n",
    "        nn = NN2Layers()\n",
    "        active_nn = nn.initiate(num_class)\n",
    "    elif model_fam == \"CNN\":\n",
    "        nn = CNN()\n",
    "        active_nn = nn.initiate(num_class)\n",
    "        \n",
    "    return active_nn\n",
    "\n",
    "# Scaling weights to the dataset proportion\n",
    "def scale_weights(num_local_samples, weights, chosen_clients):\n",
    "    num_total_samples = len(chosen_clients) * num_local_samples\n",
    "    scaling_factor = num_local_samples / num_total_samples\n",
    "\n",
    "    # Loop through each layer weight & biases\n",
    "    scaled_weights = []\n",
    "    for component in weights:\n",
    "        scaled_weights.append(scaling_factor * component)\n",
    "\n",
    "    return scaled_weights\n",
    "\n",
    "# Sum all the scaled weights from all clients\n",
    "def sum_scaled_weights(scaled_weights):\n",
    "    final_weights = []\n",
    "    for component in zip(*scaled_weights):\n",
    "        final_weights.append(tf.math.reduce_sum(component, axis=0))\n",
    "  \n",
    "    return final_weights\n",
    "\n",
    "# Custom global model evaluation\n",
    "def evaluate_model(model, test_data, test_label):\n",
    "        \n",
    "    # Predict label\n",
    "    y_pred = model.predict(test_data)\n",
    "    y_true = test_label\n",
    "\n",
    "    # Calculate loss with SCCE\n",
    "    scce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    loss = scce(y_true, y_pred).numpy()\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true, np.argmax(y_pred, axis=1))\n",
    "\n",
    "    return round(loss, 4), round(accuracy, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "995d929b-ef67-44fd-82f6-335a8e6d287b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clients_data, clients_label, x_train, y_train, x_val, y_val, x_test, y_test = \\\n",
    "generate_data(data=\"MNIST\", use_val=True, setting=\"IID\", n_clients=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5449e302-b64b-42a8-96c5-b0150c784872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 28, 28, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f936c7cc-1acc-45fa-8a51-5c71bda45b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fee1957-e2b9-46f1-83db-8c9a430899f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbbdeb7c-9879-4670-94fd-6d62c1863876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6ddbd94-4ae9-4da0-bbce-b101e3502a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"C\": 0.1, \"B\": 10, \"learning_rate\": 0.215, \"comm_round\": 1, \"train_acc\": 0.9419, \"test_acc\": 0.9454, \"train_loss\": 0.1897, \"test_loss\": 0.1775}\n",
      "{\"C\": 0.1, \"B\": 10, \"learning_rate\": 0.215, \"comm_round\": 2, \"train_acc\": 0.961, \"test_acc\": 0.9622, \"train_loss\": 0.1537, \"test_loss\": 0.138}\n",
      "{\"C\": 0.1, \"B\": 10, \"learning_rate\": 0.215, \"comm_round\": 3, \"train_acc\": 0.9681, \"test_acc\": 0.9704, \"train_loss\": 0.1291, \"test_loss\": 0.1138}\n",
      "{\"C\": 0.1, \"B\": 10, \"learning_rate\": 0.215, \"comm_round\": 4, \"train_acc\": 0.9728, \"test_acc\": 0.9741, \"train_loss\": 0.1073, \"test_loss\": 0.0954}\n",
      "{\"C\": 0.1, \"B\": 10, \"learning_rate\": 0.215, \"comm_round\": 5, \"train_acc\": 0.9762, \"test_acc\": 0.9773, \"train_loss\": 0.0928, \"test_loss\": 0.0809}\n",
      "{\"C\": 0.1, \"B\": 10, \"learning_rate\": 0.215, \"comm_round\": 6, \"train_acc\": 0.9786, \"test_acc\": 0.9796, \"train_loss\": 0.0821, \"test_loss\": 0.0729}\n",
      "{\"C\": 0.1, \"B\": 10, \"learning_rate\": 0.215, \"comm_round\": 7, \"train_acc\": 0.9805, \"test_acc\": 0.9798, \"train_loss\": 0.0755, \"test_loss\": 0.0679}\n",
      "{\"C\": 0.1, \"B\": 10, \"learning_rate\": 0.215, \"comm_round\": 8, \"train_acc\": 0.982, \"test_acc\": 0.9815, \"train_loss\": 0.0672, \"test_loss\": 0.0603}\n",
      "{\"C\": 0.1, \"B\": 10, \"learning_rate\": 0.215, \"comm_round\": 9, \"train_acc\": 0.9828, \"test_acc\": 0.9823, \"train_loss\": 0.0641, \"test_loss\": 0.0581}\n",
      "{\"C\": 0.1, \"B\": 10, \"learning_rate\": 0.215, \"comm_round\": 10, \"train_acc\": 0.9836, \"test_acc\": 0.9838, \"train_loss\": 0.0613, \"test_loss\": 0.0526}\n",
      "{\"C\": 0.1, \"B\": 10, \"learning_rate\": 0.215, \"comm_round\": 11, \"train_acc\": 0.9845, \"test_acc\": 0.984, \"train_loss\": 0.0575, \"test_loss\": 0.0503}\n",
      "{\"C\": 0.1, \"B\": 10, \"learning_rate\": 0.215, \"comm_round\": 12, \"train_acc\": 0.9853, \"test_acc\": 0.9861, \"train_loss\": 0.0534, \"test_loss\": 0.0472}\n",
      "{\"C\": 0.1, \"B\": 10, \"learning_rate\": 0.215, \"comm_round\": 13, \"train_acc\": 0.9864, \"test_acc\": 0.9853, \"train_loss\": 0.0506, \"test_loss\": 0.0476}\n",
      "{\"C\": 0.1, \"B\": 10, \"learning_rate\": 0.215, \"comm_round\": 14, \"train_acc\": 0.9869, \"test_acc\": 0.9861, \"train_loss\": 0.0491, \"test_loss\": 0.0468}\n",
      "{\"C\": 0.1, \"B\": 10, \"learning_rate\": 0.215, \"comm_round\": 15, \"train_acc\": 0.9874, \"test_acc\": 0.9863, \"train_loss\": 0.0471, \"test_loss\": 0.0459}\n",
      "{\"C\": 0.1, \"B\": 10, \"learning_rate\": 0.215, \"comm_round\": 16, \"train_acc\": 0.9878, \"test_acc\": 0.9879, \"train_loss\": 0.0448, \"test_loss\": 0.0415}\n",
      "{\"C\": 0.1, \"B\": 10, \"learning_rate\": 0.215, \"comm_round\": 17, \"train_acc\": 0.9884, \"test_acc\": 0.9876, \"train_loss\": 0.0438, \"test_loss\": 0.0415}\n",
      "{\"C\": 0.1, \"B\": 10, \"learning_rate\": 0.215, \"comm_round\": 18, \"train_acc\": 0.9886, \"test_acc\": 0.9879, \"train_loss\": 0.0413, \"test_loss\": 0.0397}\n",
      "{\"C\": 0.1, \"B\": 10, \"learning_rate\": 0.215, \"comm_round\": 19, \"train_acc\": 0.9893, \"test_acc\": 0.9883, \"train_loss\": 0.0394, \"test_loss\": 0.0397}\n",
      "{\"C\": 0.1, \"B\": 10, \"learning_rate\": 0.215, \"comm_round\": 20, \"train_acc\": 0.9895, \"test_acc\": 0.9884, \"train_loss\": 0.0381, \"test_loss\": 0.0382}\n",
      "{\"C\": 0.1, \"B\": 10, \"learning_rate\": 0.215, \"comm_round\": 21, \"train_acc\": 0.9898, \"test_acc\": 0.9888, \"train_loss\": 0.0399, \"test_loss\": 0.0402}\n",
      "{\"C\": 0.1, \"B\": 10, \"learning_rate\": 0.215, \"comm_round\": 22, \"train_acc\": 0.99, \"test_acc\": 0.989, \"train_loss\": 0.0375, \"test_loss\": 0.0387}\n",
      "{\"C\": 0.1, \"B\": 10, \"learning_rate\": 0.215, \"comm_round\": 23, \"train_acc\": 0.9906, \"test_acc\": 0.9886, \"train_loss\": 0.035, \"test_loss\": 0.0379}\n",
      "{\"C\": 0.1, \"B\": 10, \"learning_rate\": 0.215, \"comm_round\": 24, \"train_acc\": 0.9905, \"test_acc\": 0.9894, \"train_loss\": 0.0355, \"test_loss\": 0.0394}\n",
      "{\"C\": 0.1, \"B\": 10, \"learning_rate\": 0.215, \"comm_round\": 25, \"train_acc\": 0.9912, \"test_acc\": 0.9895, \"train_loss\": 0.0334, \"test_loss\": 0.0374}\n",
      "{\"C\": 0.1, \"B\": 10, \"learning_rate\": 0.215, \"comm_round\": 26, \"train_acc\": 0.9916, \"test_acc\": 0.9898, \"train_loss\": 0.0323, \"test_loss\": 0.0362}\n",
      "{\"C\": 0.1, \"B\": 10, \"learning_rate\": 0.215, \"comm_round\": 27, \"train_acc\": 0.992, \"test_acc\": 0.9904, \"train_loss\": 0.03, \"test_loss\": 0.0343}\n"
     ]
    }
   ],
   "source": [
    "file = open(\"logs_q2/lr0.215/untitled.txt\")\n",
    "for line in file.readlines():\n",
    "    if line.startswith(\"{'C'\"):\n",
    "        print(json.dumps(eval(line)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb2d411-9e5b-4eb7-9074-7861ffc9b3cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
