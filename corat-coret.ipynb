{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "443151a2-3dcd-46d6-bf57-3f1c86ad5d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import utils\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from time import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9362577b-a4d9-4d74-9bf7-b5f2043b3df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data based on chosen setting\n",
    "def generate_data(data, use_val, setting, n_clients):\n",
    "     \n",
    "    # Scale image to [0,1]\n",
    "    def scale_image(input_array):\n",
    "        input_array = input_array / 255.0\n",
    "        \n",
    "        return input_array\n",
    "    \n",
    "    # Load data from Tensorflow Library\n",
    "    if data == \"MNIST\":\n",
    "        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "    \n",
    "    # Scale data\n",
    "    x_train, x_test = scale_image(x_train), scale_image(x_test)\n",
    "\n",
    "    # Expand dims to add batch axis\n",
    "    x_train = np.expand_dims(x_train, axis=-1)\n",
    "    x_test = np.expand_dims(x_test, axis=-1)\n",
    "    \n",
    "    # Divide train into val\n",
    "    if use_val == True:\n",
    "        x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=len(x_test), random_state=42)\n",
    "    else:\n",
    "        x_val, y_val = None, None\n",
    "    \n",
    "    # Distribute data based on setting\n",
    "    if setting == \"IID\":\n",
    "        # Shuffle train data\n",
    "        x_train_shuffled, y_train_shuffled = utils.shuffle(x_train, y_train, random_state=21)\n",
    "        \n",
    "        # Partitioned proxy data to clients, each receiving 600 examples\n",
    "        clients_data, clients_label = {}, {}\n",
    "        list_data = np.array_split(x_train_shuffled, n_clients)\n",
    "        list_label = np.array_split(y_train_shuffled, n_clients)\n",
    "\n",
    "        # Distribute the data to all clients\n",
    "        for i in range(1, len(list_data)+1):\n",
    "            clients_data[\"client_%s\" % i] = list_data[i-1]\n",
    "            clients_label[\"client_%s\" % i] = list_label[i-1]\n",
    "            \n",
    "    elif setting == \"NONIID\":\n",
    "        # Sort the train data based on digits label\n",
    "        sorted_index = np.argsort(y_train)\n",
    "        y_train = y_train[sorted_index]\n",
    "        x_train = x_train[sorted_index]\n",
    "        \n",
    "        # Split training data into to distribute to 100 clients\n",
    "        data_shards = np.array_split(x_train, 100)\n",
    "        label_shards = np.array_split(y_train, 100)\n",
    "        \n",
    "        # Distribute shards to all clients\n",
    "        N_CLIENTS = 100\n",
    "        clients_data = {}\n",
    "        clients_label = {}\n",
    "        for i in range(1, N_CLIENTS+1):\n",
    "            clients_data[\"client_%s\" % i] = data_shards[i-1]\n",
    "            clients_label[\"client_%s\" % i] = label_shards[i-1]\n",
    "            \n",
    "    return clients_data, clients_label, x_train, y_train, x_val, y_val, x_test, y_test\n",
    "\n",
    "# Initiate global model\n",
    "def initiate_model(model_fam, num_class):\n",
    "    if model_fam == \"2NN\":\n",
    "        nn = NN2Layers()\n",
    "        active_nn = nn.initiate(num_class)\n",
    "    elif model_fam == \"CNN\":\n",
    "        nn = CNN()\n",
    "        active_nn = nn.initiate(num_class)\n",
    "        \n",
    "    return active_nn\n",
    "\n",
    "# Scaling weights to the dataset proportion\n",
    "def scale_weights(num_local_samples, weights, chosen_clients):\n",
    "    num_total_samples = len(chosen_clients) * num_local_samples\n",
    "    scaling_factor = num_local_samples / num_total_samples\n",
    "\n",
    "    # Loop through each layer weight & biases\n",
    "    scaled_weights = []\n",
    "    for component in weights:\n",
    "        scaled_weights.append(scaling_factor * component)\n",
    "\n",
    "    return scaled_weights\n",
    "\n",
    "# Sum all the scaled weights from all clients\n",
    "def sum_scaled_weights(scaled_weights):\n",
    "    final_weights = []\n",
    "    for component in zip(*scaled_weights):\n",
    "        final_weights.append(tf.math.reduce_sum(component, axis=0))\n",
    "  \n",
    "    return final_weights\n",
    "\n",
    "# Custom global model evaluation\n",
    "def evaluate_model(model, test_data, test_label):\n",
    "        \n",
    "    # Predict label\n",
    "    y_pred = model.predict(test_data)\n",
    "    y_true = test_label\n",
    "\n",
    "    # Calculate loss with SCCE\n",
    "    scce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    loss = scce(y_true, y_pred).numpy()\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true, np.argmax(y_pred, axis=1))\n",
    "\n",
    "    return round(loss, 4), round(accuracy, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "995d929b-ef67-44fd-82f6-335a8e6d287b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clients_data, clients_label, x_train, y_train, x_val, y_val, x_test, y_test = \\\n",
    "generate_data(data=\"MNIST\", use_val=True, setting=\"IID\", n_clients=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5449e302-b64b-42a8-96c5-b0150c784872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 28, 28, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f936c7cc-1acc-45fa-8a51-5c71bda45b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fee1957-e2b9-46f1-83db-8c9a430899f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
